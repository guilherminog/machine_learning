{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minha-primeira-Deep-learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPUbkcY8E2DIwryWNLDg9q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ckkS9BG9Zu-P"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.ToTensor() #definido a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)#Carrega parte de validação do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #Cria buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)#Carrega parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) #Cria buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "flm_6qy_a8Cj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "-ykDjO0AeKKr",
        "outputId": "505a96c3-c986-447c-dfbc-9554ec98e7e9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5618b24c10>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(imagens[0].shape)#Verificar tam do tensor de cada img\n",
        "print(etiquetas[0].shape) #Verificar a dimensão do tensor da etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQqkz0iMe_5B",
        "outputId": "46293c2f-ae27-4b54-938a-14b3a044071c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Estrutura da rede inception\n",
        "class Modelo(nn.Module):\n",
        "  def _init_(self):\n",
        "      super(Modelo, self)._init_()\n",
        "      self.linear1 = nn.Linear(28*28, 128) #camada de entrada, 784 neurônios que se ligam a 128\n",
        "      self.linear2 = nn.Linear(128, 64) #camada interna 1, 128 neurônios que se ligam a 64\n",
        "      self.linear1 = nn.Linear(64, 10) #camada interna 2, 64 neurônios que se ligam a 10\n",
        "      # para a camada de saída não é necessário nada, pois só precisamos pegar o output da cama interna 2\n",
        "      #Quanto maior camadas, maior o desempenho, consequentemente, maior gasto de processamento\n",
        "\n",
        "def forward(self,X):\n",
        "  X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a cama interna 1\n",
        "  X = F.relu(self.linear2(X)) # função de ativação da camada de entrada para a cama interna 2\n",
        "  X = self.linear3(X) # função de ativação da camada interna 2 para a cama de saída, nesse caso f(x) = x\n",
        "  return F.log_softmax(X, dim=1) # dados utilizados para calcular a pera"
      ],
      "metadata": {
        "id": "7mxzj-MDfhWK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Otimizador da rede, estrutura de treinamento\n",
        "\n",
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) #Define a política de atualização dos pesos e da bias\n",
        "    inicio = time() #timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "    criterio = nn.NLLLoss() #definido o criterio para calcular a perda\n",
        "    EPOCHS = 10 #número de EPOCHS que o algoritmo rodará, um bom treinamento seria pelo menos 100 EPOCHS\n",
        "    modelo.train() #ativando o modo de treinamento do modelo\n",
        "\n",
        "    for spoch in range(EPOCHS):\n",
        "        perda_acumulada = 0 #Inicialização de perda acumulada da epoch em questão\n",
        "\n",
        "        for imagens, etiquetas in trainloader:\n",
        "        \n",
        "            imagens = imagens.view(imagens.shape[0], -1) #Convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com a\n",
        "            otimizador.zero_grad() #zerando os gradientes por conta do ciclo anterior\n",
        "\n",
        "            output = modelo(imagens.to(device)) #colocando os dados no modelo\n",
        "            perda_instantanea = criterio(output, etiquetas.to(device)) # Calculando a perda da epoch em questão\n",
        "\n",
        "            perda_instantanea.backward() #back propagation a partir da perda\n",
        "\n",
        "            otimizador.step() #Atualizando os pesos e bias\n",
        "\n",
        "            perda_acumulada += perda_instantanea.item() #atualização de perda acumulada\n",
        "\n",
        "    else:\n",
        "        print(\"Epoch {} - Perda resultante. {}\".format(epoch+1, perda_acumulada/len(trainloader)))\n",
        "    print(\"\\n Informe do treino(em minutos) = \", (timer)-inicio/60)"
      ],
      "metadata": {
        "id": "_9IiHRHl42nB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Algoritmo de validação\n",
        "\n",
        "def validacao(modelo, valloader, device):\n",
        "    conta_corretas = 0\n",
        "    conta_todas = 0\n",
        "    for imagens, etiquetas in valloader:\n",
        "      for i in range(len(etiquetas)):\n",
        "        img = imagens[i].view(1, 784)\n",
        "        #Desativar o autograd para acelerar a validação, Grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "        with torch.no_grad():\n",
        "            logps = modelo(img.to(device)) #output do modelo em escala logaritmica\n",
        "\n",
        "        ps = torch.exp(logps)   #Converte output para escala normal(Lembrando que é um tensor)\n",
        "        probab = list(pc.cpu().numpy()[0])\n",
        "        etiqueta_pred = probab.index(max(probab)) #converte o tensor em um número, no caso, o número que o modelo previu\n",
        "        eitqueta_curta = etiquetas.numpy()[i]\n",
        "        if(etiqueta_curta == etiqueta_prod): #Compare a previsão com o valor correto\n",
        "          conta_corretas += 1\n",
        "        conta_todas += 1\n",
        "    print(\"Total de imagens testadas =\", conta_todas)\n",
        "    print(\"Precisão do modelo = {}%\", format(conta_corretas*100/conta_todas))"
      ],
      "metadata": {
        "id": "dI5rxEOZ4nGb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = Modelo()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#Verificando se existe uma placa cuda, a unica placa que possui suporte ao cuda é a NVIDIA"
      ],
      "metadata": {
        "id": "qN8hPZFkGAIE"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}